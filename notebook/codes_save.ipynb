{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee59dd74",
   "metadata": {},
   "source": [
    "## This code Create columns like AI_Suggestions, AI_Insights, AI_Drawbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc6474",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------------------\n",
    "# ORIGINAL CODE\n",
    "#--------------------------------------------------\n",
    "\n",
    "\n",
    "import pandas as pd, requests, re, os, time, json\n",
    "from math import ceil\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SERVER_BATCH = \"http://127.0.0.1:7860/batch_generate\"\n",
    "HEALTH = \"http://127.0.0.1:7860/health\"\n",
    "OUTPUT_CSV = r\"E:\\AJAY\\powerbi_codes\\llm_suggestions.csv\"\n",
    "TIMEOUT = 600\n",
    "BATCH_SIZE = 1\n",
    "MAX_TOKENS = 32\n",
    "TEMPERATURE = 0.5\n",
    "RETRIES = 2\n",
    "RETRY_WAIT = 2\n",
    "# Sentiment thresholds you provided\n",
    "NEG_THRESH = -0.1\n",
    "POS_THRESH = 0.1\n",
    "# ----------------------------\n",
    "\n",
    "# Attempt to import VADER (if not present, we'll fallback)\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    VADER_AVAILABLE = True\n",
    "except Exception:\n",
    "    VADER_AVAILABLE = False\n",
    "\n",
    "# ----------------------------\n",
    "# helper: safely extract values from a JSON-like string or raw dict (unchanged from prior)\n",
    "def extract_from_raw(raw_text, key):\n",
    "    if raw_text is None:\n",
    "        return \"\"\n",
    "    s = str(raw_text).strip()\n",
    "    try:\n",
    "        j = json.loads(s)\n",
    "        if isinstance(j, dict) and key in j:\n",
    "            return str(j.get(key) or \"\").strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    pattern = r'[\"\\']?\\b' + re.escape(key) + r'\\b[\"\\']?\\s*[:=]\\s*[\"\\']([^\"\\']+)[\"\\']'\n",
    "    m = re.search(pattern, s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    idx = s.lower().find(key.lower())\n",
    "    if idx != -1:\n",
    "        sub = s[idx: idx + 600]\n",
    "        m2 = re.search(r'[\"\\']([^\"\\']{1,600})[\"\\']', sub)\n",
    "        if m2:\n",
    "            return m2.group(1).strip()\n",
    "        m3 = re.search(r'\\b' + re.escape(key) + r'\\b\\s*[:=]\\s*([^,}]+)', sub, flags=re.IGNORECASE)\n",
    "        if m3:\n",
    "            return m3.group(1).strip().strip('\"\\' ')\n",
    "    return \"\"\n",
    "\n",
    "# helper: aggressively clean candidate string (remove labels, quotes, braces, stray words)\n",
    "def clean_candidate(val, raw=None, key=None):\n",
    "    if val is None:\n",
    "        val = \"\"\n",
    "    val = str(val).strip()\n",
    "    if (not val) and raw:\n",
    "        out = extract_from_raw(raw, key)\n",
    "        if out:\n",
    "            val = out\n",
    "    if key and re.search(r'\\b' + re.escape(key) + r'\\b', val, flags=re.IGNORECASE):\n",
    "        out = extract_from_raw(val, key)\n",
    "        if out:\n",
    "            val = out\n",
    "    val = re.sub(r'^[\\{\\[\\]\\}\\s,]+', '', val)\n",
    "    val = re.sub(r'^\\s*[\"\\']?\\s*' + re.escape(key) + r'\\s*[\"\\']?\\s*[:=\\-]\\s*', '', val, flags=re.IGNORECASE)\n",
    "    val = re.sub(r'[\"\\']?\\s*' + re.escape(key) + r'\\s*[\"\\']?\\s*[:=\\-]\\s*', '', val, flags=re.IGNORECASE)\n",
    "    val = val.strip(' \\'\"{},:')\n",
    "    val = re.sub(r'^\\s*(suggestion|insight|drawback)\\s*[:\\-–—]\\s*', '', val, flags=re.IGNORECASE)\n",
    "    if ('\"' in val or \"{\" in val) and len(val) < 2000:\n",
    "        m = re.search(r'[\"\\']([^\"\\']{2,400})[\"\\']', val)\n",
    "        if m:\n",
    "            candidate = m.group(1).strip()\n",
    "            if candidate.lower() != (key or \"\").lower():\n",
    "                val = candidate\n",
    "    val = re.sub(r'\\s+', ' ', val).strip()\n",
    "    if len(val) > 500:\n",
    "        val = val[:497] + \"...\"\n",
    "    return val\n",
    "\n",
    "# ----------------------------\n",
    "df = dataset.copy()\n",
    "\n",
    "if 'Feedback' not in df.columns:\n",
    "    # no feedback -> return dataset with chosen columns present but empty\n",
    "    df['AI_Suggestion'] = ''\n",
    "    df['AI_insights'] = ''\n",
    "    df['AI_Drawbacks'] = ''\n",
    "    df['sentiment_score'] = float('nan')\n",
    "    df['sentiment_label'] = 'unknown'\n",
    "    df['Positive_Token_Count'] = 0\n",
    "    df['Negative_Token_Count'] = 0\n",
    "    dataset = df\n",
    "else:\n",
    "    # reset & stable RowID for resume mapping\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['RowID'] = df.index\n",
    "\n",
    "    # -------- SENTIMENT (VADER) & token counts --------\n",
    "    # If available, use VADER to compute compound score and token lexicon counts\n",
    "    if VADER_AVAILABLE:\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        # compute compound score for each feedback\n",
    "        df['sentiment_score'] = df['Feedback'].astype(str).apply(lambda t: analyzer.polarity_scores(str(t))['compound'])\n",
    "        # map to label using provided thresholds\n",
    "        def _label_from_score(x):\n",
    "            try:\n",
    "                if pd.isna(x):\n",
    "                    return 'unknown'\n",
    "                if x < NEG_THRESH:\n",
    "                    return 'negative'\n",
    "                if x > POS_THRESH:\n",
    "                    return 'positive'\n",
    "                return 'neutral'\n",
    "            except Exception:\n",
    "                return 'unknown'\n",
    "        df['sentiment_label'] = df['sentiment_score'].apply(_label_from_score)\n",
    "\n",
    "        # Token-level counts using VADER lexicon (counts only tokens present in lexicon and with +/-\n",
    "        lex = analyzer.lexicon  # dict[word] -> valence\n",
    "        # Preprocess lex keys to set for fast membership tests\n",
    "        lex_pos = {w for w,v in lex.items() if v > 0}\n",
    "        lex_neg = {w for w,v in lex.items() if v < 0}\n",
    "        # tokenization helper: split on non-word characters, lower-case\n",
    "        token_pattern = re.compile(r\"[A-Za-z0-9']+\")\n",
    "        def count_tokens(text):\n",
    "            t = str(text).lower()\n",
    "            tokens = token_pattern.findall(t)\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            # count only true lexicon matches\n",
    "            for tok in tokens:\n",
    "                if tok in lex_pos:\n",
    "                    pos += 1\n",
    "                elif tok in lex_neg:\n",
    "                    neg += 1\n",
    "            return pos, neg\n",
    "\n",
    "        counts = df['Feedback'].astype(str).apply(count_tokens)\n",
    "        df['Positive_Token_Count'] = counts.apply(lambda x: int(x[0]) if isinstance(x, (list,tuple)) else 0)\n",
    "        df['Negative_Token_Count'] = counts.apply(lambda x: int(x[1]) if isinstance(x, (list,tuple)) else 0)\n",
    "\n",
    "    else:\n",
    "        # VADER not installed: set safe defaults or simple fallback\n",
    "        df['sentiment_score'] = float('nan')\n",
    "        df['sentiment_label'] = 'unknown'\n",
    "        df['Positive_Token_Count'] = 0\n",
    "        df['Negative_Token_Count'] = 0\n",
    "\n",
    "    # -------- LLM / AI columns (resumable cache) --------\n",
    "    # quick health check: if server unreachable, still return with sentiment columns present\n",
    "    try:\n",
    "        _ = requests.get(HEALTH, timeout=5)\n",
    "        server_up = True\n",
    "    except Exception:\n",
    "        server_up = False\n",
    "\n",
    "    if not server_up:\n",
    "        # merge empty AI columns and return quickly (keep sentiment columns)\n",
    "        df['AI_Suggestion'] = ''\n",
    "        df['AI_insights'] = ''\n",
    "        df['AI_Drawbacks'] = ''\n",
    "        dataset = df.drop(columns=['RowID'])\n",
    "    else:\n",
    "        # existing LLM/resume logic (kept same; uses OUTPUT_CSV for caching)\n",
    "        def make_prompt(text):\n",
    "            t = re.sub(r'[\\r\\n]+',' ', str(text)).strip()[:300]\n",
    "            return (\n",
    "                \"You are a concise feedback analyst. RETURN ONLY a single-line JSON object EXACTLY like: \"\n",
    "                '{\"suggestion\":\"...\",\"insight\":\"...\",\"drawback\":\"...\"} . '\n",
    "                \"Each value <= 12 words. No extra commentary. Respond with JSON only.\\n\\n\"\n",
    "                f\"Feedback: \\\"{t}\\\"\"\n",
    "            )\n",
    "\n",
    "        existing = None\n",
    "        if os.path.exists(OUTPUT_CSV):\n",
    "            try:\n",
    "                existing = pd.read_csv(OUTPUT_CSV)\n",
    "                if 'RowID' not in existing.columns:\n",
    "                    existing = None\n",
    "            except Exception:\n",
    "                existing = None\n",
    "\n",
    "        n = len(df)\n",
    "        res = pd.DataFrame({'RowID': df['RowID'], 'AI_Suggestion': ['']*n, 'AI_insights': ['']*n, 'AI_Drawbacks': ['']*n})\n",
    "\n",
    "        if existing is not None:\n",
    "            existing = existing[existing['RowID'].isin(df['RowID'])]\n",
    "            existing = existing.set_index('RowID')\n",
    "            for col in ['AI_Suggestion','AI_insights','AI_Drawbacks']:\n",
    "                if col in existing.columns:\n",
    "                    for rid in existing.index:\n",
    "                        try:\n",
    "                            res.loc[res['RowID']==rid, col] = str(existing.loc[rid, col] or \"\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "        to_process = [int(x) for x in res.loc[res['AI_Suggestion'].fillna('') == '', 'RowID'].tolist()]\n",
    "\n",
    "        if to_process:\n",
    "            total = len(to_process)\n",
    "            batches = ceil(total / BATCH_SIZE)\n",
    "            for b in range(batches):\n",
    "                batch_idxs = to_process[b*BATCH_SIZE : (b+1)*BATCH_SIZE]\n",
    "                if not batch_idxs:\n",
    "                    continue\n",
    "                items = []\n",
    "                for rid in batch_idxs:\n",
    "                    fb = df.at[rid, 'Feedback']\n",
    "                    items.append({\"prompt\": make_prompt(fb), \"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE})\n",
    "                payload = {\"items\": items}\n",
    "\n",
    "                body = None\n",
    "                last_err = None\n",
    "                for attempt in range(1, RETRIES+1):\n",
    "                    try:\n",
    "                        r = requests.post(SERVER_BATCH, json=payload, timeout=TIMEOUT)\n",
    "                        r.raise_for_status()\n",
    "                        body = r.json()\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        last_err = str(e)\n",
    "                        time.sleep(RETRY_WAIT)\n",
    "                if body is None:\n",
    "                    for rid in batch_idxs:\n",
    "                        res.loc[res['RowID']==rid, ['AI_Suggestion','AI_insights','AI_Drawbacks']] = ['', '', '']\n",
    "                    try:\n",
    "                        res.to_csv(OUTPUT_CSV, index=False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    continue\n",
    "\n",
    "                results = body.get('results') if isinstance(body, dict) and 'results' in body else body\n",
    "                if not results:\n",
    "                    for rid in batch_idxs:\n",
    "                        res.loc[res['RowID']==rid, ['AI_Suggestion','AI_insights','AI_Drawbacks']] = ['', '', '']\n",
    "                    try:\n",
    "                        res.to_csv(OUTPUT_CSV, index=False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    continue\n",
    "\n",
    "                for j, resp in enumerate(results):\n",
    "                    rid = batch_idxs[j]\n",
    "                    raw_txt = ''\n",
    "                    s_raw = i_raw = d_raw = ''\n",
    "                    if isinstance(resp, dict):\n",
    "                        s_raw = resp.get('suggestion','') or ''\n",
    "                        i_raw = resp.get('insight','') or ''\n",
    "                        d_raw = resp.get('drawback','') or ''\n",
    "                        raw_txt = resp.get('raw','') or resp.get('text','') or ''\n",
    "                    else:\n",
    "                        raw_txt = str(resp)\n",
    "\n",
    "                    s_val = clean_candidate(s_raw, raw=raw_txt, key='suggestion')\n",
    "                    i_val = clean_candidate(i_raw, raw=raw_txt, key='insight')\n",
    "                    d_val = clean_candidate(d_raw, raw=raw_txt, key='drawback')\n",
    "\n",
    "                    if not s_val:\n",
    "                        s_val = extract_from_raw(raw_txt, 'suggestion')\n",
    "                    if not i_val:\n",
    "                        i_val = extract_from_raw(raw_txt, 'insight')\n",
    "                    if not d_val:\n",
    "                        d_val = extract_from_raw(raw_txt, 'drawback')\n",
    "\n",
    "                    if (not s_val or not i_val or not d_val) and raw_txt:\n",
    "                        try:\n",
    "                            parsed = json.loads(raw_txt)\n",
    "                            if isinstance(parsed, dict):\n",
    "                                if not s_val:\n",
    "                                    s_val = str(parsed.get('suggestion','') or parsed.get('SUGGESTION','') or '')\n",
    "                                if not i_val:\n",
    "                                    i_val = str(parsed.get('insight','') or parsed.get('INSIGHT','') or '')\n",
    "                                if not d_val:\n",
    "                                    d_val = str(parsed.get('drawback','') or parsed.get('DRAWBACK','') or '')\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    if not (s_val or i_val or d_val):\n",
    "                        lines = [ln.strip() for ln in str(raw_txt).splitlines() if ln.strip()]\n",
    "                        if lines:\n",
    "                            if not s_val:\n",
    "                                s_val = lines[0]\n",
    "                            if len(lines) > 1 and not i_val:\n",
    "                                i_val = lines[1]\n",
    "                            if len(lines) > 2 and not d_val:\n",
    "                                d_val = lines[2]\n",
    "\n",
    "                    def finalize(x):\n",
    "                        if x is None:\n",
    "                            return ''\n",
    "                        t = str(x).strip()\n",
    "                        t = re.sub(r'^\\s*(suggestion|insight|drawback)\\b\\s*[:\\-–—]\\s*', '', t, flags=re.IGNORECASE)\n",
    "                        t = t.strip(' \\'\"{},:')\n",
    "                        t = re.sub(r'\\s+', ' ', t).strip()\n",
    "                        if len(t) > 500:\n",
    "                            t = t[:497] + '...'\n",
    "                        return t\n",
    "\n",
    "                    res.loc[res['RowID']==rid, 'AI_Suggestion'] = finalize(s_val)\n",
    "                    res.loc[res['RowID']==rid, 'AI_insights'] = finalize(i_val)\n",
    "                    res.loc[res['RowID']==rid, 'AI_Drawbacks'] = finalize(d_val)\n",
    "\n",
    "                try:\n",
    "                    res.to_csv(OUTPUT_CSV, index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # merge back into df\n",
    "        res = res.set_index('RowID')\n",
    "        df['AI_Suggestion'] = df['RowID'].map(lambda x: res.at[x, 'AI_Suggestion'] if x in res.index else '')\n",
    "        df['AI_insights'] = df['RowID'].map(lambda x: res.at[x, 'AI_insights'] if x in res.index else '')\n",
    "        df['AI_Drawbacks'] = df['RowID'].map(lambda x: res.at[x, 'AI_Drawbacks'] if x in res.index else '')\n",
    "\n",
    "        # remove helper RowID and return\n",
    "        dataset = df.drop(columns=['RowID'])\n",
    "\n",
    "# End — Power BI receives 'dataset' containing:\n",
    "# original columns + AI_Suggestion, AI_insights, AI_Drawbacks, sentiment_score, sentiment_label, Positive_Token_Count, Negative_Token_Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da7458",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137d43ca",
   "metadata": {},
   "source": [
    "## This below code create the Service, Staff, Cost, Cleanliness, Communication, Management, Facility, Safety, and Experience. and AI columsn also well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e0b8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Power BI \n",
    "#columns and return single unified dataset \n",
    "\n",
    "import pandas as pd, requests, re, os, time, json\n",
    "from math import ceil\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SERVER_BATCH = \"http://127.0.0.1:7860/batch_generate\"\n",
    "HEALTH = \"http://127.0.0.1:7860/health\"\n",
    "OUTPUT_CSV = r\"E:\\AJAY\\powerbi_codes\\llm_suggestions.csv\"\n",
    "TIMEOUT = 600\n",
    "BATCH_SIZE = 1\n",
    "MAX_TOKENS = 64\n",
    "TEMPERATURE = 0.0\n",
    "RETRIES = 2\n",
    "RETRY_WAIT = 2\n",
    "# Sentiment thresholds you provided\n",
    "NEG_THRESH = -0.1\n",
    "POS_THRESH = 0.1\n",
    "# ----------------------------\n",
    "\n",
    "# Attempt to import VADER (if not present, we'll fallback)\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    VADER_AVAILABLE = True\n",
    "except Exception:\n",
    "    VADER_AVAILABLE = False\n",
    "\n",
    "# ----------------------------\n",
    "# helper: safely extract values from a JSON-like string or raw dict (unchanged from prior)\n",
    "def extract_from_raw(raw_text, key):\n",
    "    if raw_text is None:\n",
    "        return \"\"\n",
    "    s = str(raw_text).strip()\n",
    "    try:\n",
    "        j = json.loads(s)\n",
    "        if isinstance(j, dict) and key in j:\n",
    "            return str(j.get(key) or \"\").strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    pattern = r'[\"\\']?\\b' + re.escape(key) + r'\\b[\"\\']?\\s*[:=]\\s*[\"\\']([^\"\\']+)[\"\\']'\n",
    "    m = re.search(pattern, s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    idx = s.lower().find(key.lower())\n",
    "    if idx != -1:\n",
    "        sub = s[idx: idx + 600]\n",
    "        m2 = re.search(r'[\"\\']([^\"\\']{1,600})[\"\\']', sub)\n",
    "        if m2:\n",
    "            return m2.group(1).strip()\n",
    "        m3 = re.search(r'\\b' + re.escape(key) + r'\\b\\s*[:=]\\s*([^,}]+)', sub, flags=re.IGNORECASE)\n",
    "        if m3:\n",
    "            return m3.group(1).strip().strip('\"\\' ')\n",
    "    return \"\"\n",
    "\n",
    "# helper: aggressively clean candidate string (remove labels, quotes, braces, stray words)\n",
    "def clean_candidate(val, raw=None, key=None):\n",
    "    if val is None:\n",
    "        val = \"\"\n",
    "    val = str(val).strip()\n",
    "    if (not val) and raw:\n",
    "        out = extract_from_raw(raw, key)\n",
    "        if out:\n",
    "            val = out\n",
    "    if key and re.search(r'\\b' + re.escape(key) + r'\\b', val, flags=re.IGNORECASE):\n",
    "        out = extract_from_raw(val, key)\n",
    "        if out:\n",
    "            val = out\n",
    "    val = re.sub(r'^[\\{\\[\\]\\}\\s,]+', '', val)\n",
    "    val = re.sub(r'^\\s*[\"\\']?\\s*' + re.escape(key) + r'\\s*[\"\\']?\\s*[:=\\-]\\s*', '', val, flags=re.IGNORECASE)\n",
    "    val = re.sub(r'[\"\\']?\\s*' + re.escape(key) + r'\\s*[\"\\']?\\s*[:=\\-]\\s*', '', val, flags=re.IGNORECASE)\n",
    "    val = val.strip(' \\'\"{},:')\n",
    "    val = re.sub(r'^\\s*(suggestion|insight|drawback)\\s*[:\\-–—]\\s*', '', val, flags=re.IGNORECASE)\n",
    "    if ('\"' in val or \"{\" in val) and len(val) < 2000:\n",
    "        m = re.search(r'[\"\\']([^\"\\']{2,400})[\"\\']', val)\n",
    "        if m:\n",
    "            candidate = m.group(1).strip()\n",
    "            if candidate.lower() != (key or \"\").lower():\n",
    "                val = candidate\n",
    "    val = re.sub(r'\\s+', ' ', val).strip()\n",
    "    if len(val) > 500:\n",
    "        val = val[:497] + \"...\"\n",
    "    return val\n",
    "\n",
    "# normalization helper for category values\n",
    "def normalize_cat_value(v):\n",
    "    if v is None:\n",
    "        return ''\n",
    "    s = str(v).strip()\n",
    "    s_low = s.lower()\n",
    "    if 'satisf' in s_low:\n",
    "        return 'Satisfied'\n",
    "    if 'not satisfied' in s_low or ('not' in s_low and 'satisf' not in s_low and 'no' in s_low):\n",
    "        return 'Not Satisfied'\n",
    "    if 'not mentioned' in s_low or s_low in ('', 'na', 'n/a', 'none'):\n",
    "        return 'Not Mentioned'\n",
    "    # fallback heuristics\n",
    "    if s_low in ('satisfied','yes','positive','good','ok','okay'):\n",
    "        return 'Satisfied'\n",
    "    if s_low in ('negative','bad','unsatisfied','no','not ok','poor'):\n",
    "        return 'Not Satisfied'\n",
    "    return 'Not Mentioned'\n",
    "\n",
    "# categories list (order matters for rating)\n",
    "CATEGORIES = ['Service','Staff','Cost','Cleanliness','Communication','Management','Facility','Safety','Experience']\n",
    "\n",
    "# ----------------------------\n",
    "df = dataset.copy()\n",
    "\n",
    "if 'Feedback' not in df.columns:\n",
    "    # no feedback -> return dataset with chosen columns present but empty\n",
    "    df['AI_Suggestion'] = ''\n",
    "    df['AI_insights'] = ''\n",
    "    df['AI_Drawbacks'] = ''\n",
    "    df['sentiment_score'] = float('nan')\n",
    "    df['sentiment_label'] = 'unknown'\n",
    "    df['Positive_Token_Count'] = 0\n",
    "    df['Negative_Token_Count'] = 0\n",
    "    for cat in CATEGORIES:\n",
    "        df[cat] = 'Not Mentioned'\n",
    "    df['Overall_Experience_Rating'] = float('nan')\n",
    "    dataset = df\n",
    "else:\n",
    "    # reset & stable RowID for resume mapping\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['RowID'] = df.index\n",
    "\n",
    "    # -------- SENTIMENT (VADER) & token counts --------\n",
    "    if VADER_AVAILABLE:\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        df['sentiment_score'] = df['Feedback'].astype(str).apply(lambda t: analyzer.polarity_scores(str(t))['compound'])\n",
    "        def _label_from_score(x):\n",
    "            try:\n",
    "                if pd.isna(x):\n",
    "                    return 'unknown'\n",
    "                if x < NEG_THRESH:\n",
    "                    return 'negative'\n",
    "                if x > POS_THRESH:\n",
    "                    return 'positive'\n",
    "                return 'neutral'\n",
    "            except Exception:\n",
    "                return 'unknown'\n",
    "        df['sentiment_label'] = df['sentiment_score'].apply(_label_from_score)\n",
    "\n",
    "        lex = analyzer.lexicon\n",
    "        lex_pos = {w for w,v in lex.items() if v > 0}\n",
    "        lex_neg = {w for w,v in lex.items() if v < 0}\n",
    "        token_pattern = re.compile(r\"[A-Za-z0-9']+\")\n",
    "        def count_tokens(text):\n",
    "            t = str(text).lower()\n",
    "            tokens = token_pattern.findall(t)\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            for tok in tokens:\n",
    "                if tok in lex_pos:\n",
    "                    pos += 1\n",
    "                elif tok in lex_neg:\n",
    "                    neg += 1\n",
    "            return pos, neg\n",
    "        counts = df['Feedback'].astype(str).apply(count_tokens)\n",
    "        df['Positive_Token_Count'] = counts.apply(lambda x: int(x[0]) if isinstance(x, (list,tuple)) else 0)\n",
    "        df['Negative_Token_Count'] = counts.apply(lambda x: int(x[1]) if isinstance(x, (list,tuple)) else 0)\n",
    "    else:\n",
    "        df['sentiment_score'] = float('nan')\n",
    "        df['sentiment_label'] = 'unknown'\n",
    "        df['Positive_Token_Count'] = 0\n",
    "        df['Negative_Token_Count'] = 0\n",
    "\n",
    "    # -----------------------\n",
    "    # Prepare category columns (load cache if exists)\n",
    "    # -----------------------\n",
    "    # If OUTPUT_CSV exists and has RowID + category columns, use it to prefill\n",
    "    existing_cache = None\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        try:\n",
    "            existing_cache = pd.read_csv(OUTPUT_CSV)\n",
    "        except Exception:\n",
    "            existing_cache = None\n",
    "\n",
    "    # initialize category columns empty (so we can mark which need processing)\n",
    "    for cat in CATEGORIES:\n",
    "        df[cat] = ''\n",
    "\n",
    "    # If cache present and contains categories, copy by RowID\n",
    "    if existing_cache is not None and 'RowID' in existing_cache.columns:\n",
    "        # map by RowID (cache may have AI columns too)\n",
    "        try:\n",
    "            existing_cache = existing_cache.set_index('RowID')\n",
    "            for idx in df['RowID']:\n",
    "                if idx in existing_cache.index:\n",
    "                    for cat in CATEGORIES:\n",
    "                        if cat in existing_cache.columns:\n",
    "                            val = existing_cache.at[idx, cat]\n",
    "                            if pd.isna(val):\n",
    "                                val = ''\n",
    "                            df.at[idx, cat] = str(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # find rows that need classification (blank category values)\n",
    "    to_process = [int(x) for x in df.loc[df[CATEGORIES[0]].fillna('') == '', 'RowID'].tolist()]\n",
    "\n",
    "    # If there are rows and server is reachable, call LLM to classify per row (row-by-row or small batches)\n",
    "    try:\n",
    "        _ = requests.get(HEALTH, timeout=5)\n",
    "        server_up = True\n",
    "    except Exception:\n",
    "        server_up = False\n",
    "\n",
    "    if to_process and server_up:\n",
    "        # Build strict prompt template for category classification\n",
    "        def make_cat_prompt(feedback_text):\n",
    "            # LLM must return JSON object with keys exactly the category names and values exactly one of:\n",
    "            # \"Satisfied\", \"Not Satisfied\", \"Not Mentioned\"\n",
    "            t = re.sub(r'[\\r\\n]+',' ', str(feedback_text)).strip()[:800]\n",
    "            prompt = (\n",
    "                \"You are a feedback classification assistant. Analyze the single feedback text and RETURN ONLY a JSON object \"\n",
    "                \"with exactly these keys (lowercase or title case accepted): \"\n",
    "                f\"{json.dumps(CATEGORIES)}. For each key assign one of these EXACT values: \\\"Satisfied\\\", \\\"Not Satisfied\\\", or \\\"Not Mentioned\\\". \"\n",
    "                \"Do NOT include any other keys or commentary. Values must be exactly one of the three options. \"\n",
    "                \"If the feedback mentions the category positively, use \\\"Satisfied\\\"; if it clearly complains, use \\\"Not Satisfied\\\"; \"\n",
    "                \"if the feedback does not mention the category, use \\\"Not Mentioned\\\". \"\n",
    "                \"Return JSON only.\\n\\n\"\n",
    "                f\"Feedback: \\\"{t}\\\"\\n\"\n",
    "            )\n",
    "            return prompt\n",
    "\n",
    "        # We'll call the batch endpoint with items list where each item is the prompt; batch size kept small to avoid server 422s\n",
    "        # Use BATCH_SIZE configured (often 1)\n",
    "        batches = ceil(len(to_process) / BATCH_SIZE)\n",
    "        cache_rows = []\n",
    "        for b in range(batches):\n",
    "            batch_idxs = to_process[b*BATCH_SIZE : (b+1)*BATCH_SIZE]\n",
    "            items = []\n",
    "            for rid in batch_idxs:\n",
    "                fb = df.at[rid, 'Feedback']\n",
    "                items.append({\"prompt\": make_cat_prompt(fb), \"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE})\n",
    "            payload = {\"items\": items}\n",
    "            body = None\n",
    "            last_err = None\n",
    "            # try batch endpoint\n",
    "            for attempt in range(1, RETRIES+1):\n",
    "                try:\n",
    "                    r = requests.post(SERVER_BATCH, json=payload, timeout=TIMEOUT)\n",
    "                    r.raise_for_status()\n",
    "                    try:\n",
    "                        body = r.json()\n",
    "                    except Exception:\n",
    "                        body = r.text\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    last_err = str(e)\n",
    "                    time.sleep(RETRY_WAIT)\n",
    "            if body is None:\n",
    "                # mark these rows as Not Mentioned to fail-safe (do not overwrite if cache had value)\n",
    "                for rid in batch_idxs:\n",
    "                    for cat in CATEGORIES:\n",
    "                        if not df.at[rid, cat]:\n",
    "                            df.at[rid, cat] = 'Not Mentioned'\n",
    "                continue\n",
    "\n",
    "            # body may contain 'results' list or be a list/dict with response text\n",
    "            results = body.get('results') if isinstance(body, dict) and 'results' in body else body\n",
    "            # if results is dict or string, normalize to list\n",
    "            if results is None:\n",
    "                results = [body]\n",
    "            if isinstance(results, dict):\n",
    "                # maybe server returned single dict for the whole batch\n",
    "                results = [results]\n",
    "\n",
    "            # Iterate responses and extract JSON\n",
    "            for j, resp in enumerate(results):\n",
    "                rid = batch_idxs[j]\n",
    "                raw_txt = ''\n",
    "                if isinstance(resp, dict):\n",
    "                    # try common fields\n",
    "                    raw_txt = resp.get('raw') or resp.get('text') or resp.get('output') or json.dumps(resp)\n",
    "                else:\n",
    "                    raw_txt = str(resp)\n",
    "                # try to parse JSON object from raw_txt\n",
    "                parsed = None\n",
    "                try:\n",
    "                    parsed = json.loads(raw_txt)\n",
    "                except Exception:\n",
    "                    # try to find first {...} block\n",
    "                    m = re.search(r'(\\{[\\s\\S]*\\})', raw_txt)\n",
    "                    if m:\n",
    "                        try:\n",
    "                            parsed = json.loads(m.group(1))\n",
    "                        except Exception:\n",
    "                            # try replace single quotes\n",
    "                            try:\n",
    "                                parsed = json.loads(m.group(1).replace(\"'\", '\"'))\n",
    "                            except Exception:\n",
    "                                parsed = None\n",
    "                # If parsed is dict, extract category values\n",
    "                if isinstance(parsed, dict):\n",
    "                    for cat in CATEGORIES:\n",
    "                        # look for keys case-insensitively\n",
    "                        val = ''\n",
    "                        for k in (cat, cat.lower(), cat.upper()):\n",
    "                            if k in parsed:\n",
    "                                val = parsed[k]\n",
    "                                break\n",
    "                        # try any matching key ignoring case\n",
    "                        if val == '':\n",
    "                            for k in parsed.keys():\n",
    "                                if isinstance(k, str) and k.lower() == cat.lower():\n",
    "                                    val = parsed[k]\n",
    "                                    break\n",
    "                        val_norm = normalize_cat_value(val)\n",
    "                        if val_norm:\n",
    "                            df.at[rid, cat] = val_norm\n",
    "                else:\n",
    "                    # fallback heuristics: try to extract \"Service\": \"Satisfied\" like patterns via regex\n",
    "                    for cat in CATEGORIES:\n",
    "                        v = extract_from_raw(raw_txt, cat)\n",
    "                        if v:\n",
    "                            df.at[rid, cat] = normalize_cat_value(v)\n",
    "                    # if still any empty, attempt to find \"Satisfied\" / \"Not Satisfied\" in raw text per cat label\n",
    "                    for cat in CATEGORIES:\n",
    "                        if not df.at[rid, cat]:\n",
    "                            # try patterns like Service: Satisfied\n",
    "                            pat = r'(' + re.escape(cat) + r')\\s*[:\\-]\\s*(Satisfied|Not Satisfied|Not Mentioned)'\n",
    "                            m2 = re.search(pat, raw_txt, flags=re.IGNORECASE)\n",
    "                            if m2:\n",
    "                                df.at[rid, cat] = normalize_cat_value(m2.group(2))\n",
    "                # ensure all categories have a value now (fallback to Not Mentioned)\n",
    "                for cat in CATEGORIES:\n",
    "                    if not df.at[rid, cat]:\n",
    "                        df.at[rid, cat] = 'Not Mentioned'\n",
    "                # store cache row for persistence\n",
    "                cache_row = {'RowID': rid}\n",
    "                for cat in CATEGORIES:\n",
    "                    cache_row[cat] = df.at[rid, cat]\n",
    "                cache_rows.append(cache_row)\n",
    "\n",
    "        # Persist category cache into OUTPUT_CSV (merge with existing cache file preserving other columns)\n",
    "        try:\n",
    "            if os.path.exists(OUTPUT_CSV):\n",
    "                try:\n",
    "                    old = pd.read_csv(OUTPUT_CSV)\n",
    "                except Exception:\n",
    "                    old = pd.DataFrame()\n",
    "            else:\n",
    "                old = pd.DataFrame()\n",
    "            # convert cache_rows to DataFrame\n",
    "            if cache_rows:\n",
    "                new_cache_df = pd.DataFrame(cache_rows)\n",
    "                # merge on RowID (overwrite category cols)\n",
    "                if not old.empty:\n",
    "                    old = old.set_index('RowID')\n",
    "                    new_cache_df = new_cache_df.set_index('RowID')\n",
    "                    for cid in new_cache_df.index:\n",
    "                        old.loc[cid, new_cache_df.columns] = new_cache_df.loc[cid]\n",
    "                    merged = old.reset_index()\n",
    "                else:\n",
    "                    merged = new_cache_df.reset_index()\n",
    "                # save merged\n",
    "                merged.to_csv(OUTPUT_CSV, index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        # If server not up or nothing to process, fill missing categories with Not Mentioned\n",
    "        for idx, row in df.iterrows():\n",
    "            for cat in CATEGORIES:\n",
    "                if not df.at[idx, cat]:\n",
    "                    df.at[idx, cat] = 'Not Mentioned'\n",
    "\n",
    "    # -----------------------\n",
    "    # Compute Overall_Experience_Rating using specified mapping\n",
    "    # -----------------------\n",
    "    score_map = {'Satisfied': 1.0, 'Not Satisfied': 0.0, 'Not Mentioned': 0.5}\n",
    "    def compute_overall_rating_row(r):\n",
    "        s = 0.0\n",
    "        count = 0\n",
    "        for cat in CATEGORIES:\n",
    "            lab = r.get(cat, 'Not Mentioned')\n",
    "            s += score_map.get(lab, 0.5)\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            return float('nan')\n",
    "        rating = (s / count) * 5.0\n",
    "        try:\n",
    "            return round(float(rating), 1)\n",
    "        except Exception:\n",
    "            return float('nan')\n",
    "    df['Overall_Experience_Rating'] = df.apply(compute_overall_rating_row, axis=1)\n",
    "\n",
    "    # -------- LLM / AI columns (resumable cache) --------\n",
    "    # quick health check: if server unreachable, still return with sentiment columns present\n",
    "    try:\n",
    "        _ = requests.get(HEALTH, timeout=5)\n",
    "        server_up = True\n",
    "    except Exception:\n",
    "        server_up = False\n",
    "\n",
    "    if not server_up:\n",
    "        # merge empty AI columns and return quickly (keep sentiment columns)\n",
    "        df['AI_Suggestion'] = ''\n",
    "        df['AI_insights'] = ''\n",
    "        df['AI_Drawbacks'] = ''\n",
    "        dataset = df.drop(columns=['RowID'])\n",
    "    else:\n",
    "        # existing LLM/resume logic (kept same; uses OUTPUT_CSV for caching)\n",
    "        def make_prompt(text):\n",
    "            t = re.sub(r'[\\r\\n]+',' ', str(text)).strip()[:300]\n",
    "            return (\n",
    "                \"You are a concise feedback analyst. RETURN ONLY a single-line JSON object EXACTLY like: \"\n",
    "                '{\"suggestion\":\"...\",\"insight\":\"...\",\"drawback\":\"...\"} . '\n",
    "                \"Each value <= 12 words. No extra commentary. Respond with JSON only.\\n\\n\"\n",
    "                f\"Feedback: \\\"{t}\\\"\"\n",
    "            )\n",
    "\n",
    "        existing = None\n",
    "        if os.path.exists(OUTPUT_CSV):\n",
    "            try:\n",
    "                existing = pd.read_csv(OUTPUT_CSV)\n",
    "                if 'RowID' not in existing.columns:\n",
    "                    existing = None\n",
    "            except Exception:\n",
    "                existing = None\n",
    "\n",
    "        n = len(df)\n",
    "        res = pd.DataFrame({'RowID': df['RowID'], 'AI_Suggestion': ['']*n, 'AI_insights': ['']*n, 'AI_Drawbacks': ['']*n})\n",
    "\n",
    "        if existing is not None:\n",
    "            # Copy any existing AI columns present in cache\n",
    "            existing = existing[existing['RowID'].isin(df['RowID'])]\n",
    "            existing = existing.set_index('RowID')\n",
    "            for col in ['AI_Suggestion','AI_insights','AI_Drawbacks']:\n",
    "                if col in existing.columns:\n",
    "                    for rid in existing.index:\n",
    "                        try:\n",
    "                            res.loc[res['RowID']==rid, col] = str(existing.loc[rid, col] or \"\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "        to_process_ai = [int(x) for x in res.loc[res['AI_Suggestion'].fillna('') == '', 'RowID'].tolist()]\n",
    "\n",
    "        if to_process_ai:\n",
    "            total = len(to_process_ai)\n",
    "            batches = ceil(total / BATCH_SIZE)\n",
    "            for b in range(batches):\n",
    "                batch_idxs = to_process_ai[b*BATCH_SIZE : (b+1)*BATCH_SIZE]\n",
    "                if not batch_idxs:\n",
    "                    continue\n",
    "                items = []\n",
    "                for rid in batch_idxs:\n",
    "                    fb = df.at[rid, 'Feedback']\n",
    "                    items.append({\"prompt\": make_prompt(fb), \"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE})\n",
    "                payload = {\"items\": items}\n",
    "\n",
    "                body = None\n",
    "                last_err = None\n",
    "                for attempt in range(1, RETRIES+1):\n",
    "                    try:\n",
    "                        r = requests.post(SERVER_BATCH, json=payload, timeout=TIMEOUT)\n",
    "                        r.raise_for_status()\n",
    "                        body = r.json()\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        last_err = str(e)\n",
    "                        time.sleep(RETRY_WAIT)\n",
    "                if body is None:\n",
    "                    for rid in batch_idxs:\n",
    "                        res.loc[res['RowID']==rid, ['AI_Suggestion','AI_insights','AI_Drawbacks']] = ['', '', '']\n",
    "                    try:\n",
    "                        res.to_csv(OUTPUT_CSV, index=False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    continue\n",
    "\n",
    "                results = body.get('results') if isinstance(body, dict) and 'results' in body else body\n",
    "                if not results:\n",
    "                    for rid in batch_idxs:\n",
    "                        res.loc[res['RowID']==rid, ['AI_Suggestion','AI_insights','AI_Drawbacks']] = ['', '', '']\n",
    "                    try:\n",
    "                        res.to_csv(OUTPUT_CSV, index=False)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    continue\n",
    "\n",
    "                for j, resp in enumerate(results):\n",
    "                    rid = batch_idxs[j]\n",
    "                    raw_txt = ''\n",
    "                    s_raw = i_raw = d_raw = ''\n",
    "                    if isinstance(resp, dict):\n",
    "                        s_raw = resp.get('suggestion','') or ''\n",
    "                        i_raw = resp.get('insight','') or ''\n",
    "                        d_raw = resp.get('drawback','') or ''\n",
    "                        raw_txt = resp.get('raw','') or resp.get('text','') or ''\n",
    "                    else:\n",
    "                        raw_txt = str(resp)\n",
    "\n",
    "                    s_val = clean_candidate(s_raw, raw=raw_txt, key='suggestion')\n",
    "                    i_val = clean_candidate(i_raw, raw=raw_txt, key='insight')\n",
    "                    d_val = clean_candidate(d_raw, raw=raw_txt, key='drawback')\n",
    "\n",
    "                    if not s_val:\n",
    "                        s_val = extract_from_raw(raw_txt, 'suggestion')\n",
    "                    if not i_val:\n",
    "                        i_val = extract_from_raw(raw_txt, 'insight')\n",
    "                    if not d_val:\n",
    "                        d_val = extract_from_raw(raw_txt, 'drawback')\n",
    "\n",
    "                    if (not s_val or not i_val or not d_val) and raw_txt:\n",
    "                        try:\n",
    "                            parsed = json.loads(raw_txt)\n",
    "                            if isinstance(parsed, dict):\n",
    "                                if not s_val:\n",
    "                                    s_val = str(parsed.get('suggestion','') or parsed.get('SUGGESTION','') or '')\n",
    "                                if not i_val:\n",
    "                                    i_val = str(parsed.get('insight','') or parsed.get('INSIGHT','') or '')\n",
    "                                if not d_val:\n",
    "                                    d_val = str(parsed.get('drawback','') or parsed.get('DRAWBACK','') or '')\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    if not (s_val or i_val or d_val):\n",
    "                        lines = [ln.strip() for ln in str(raw_txt).splitlines() if ln.strip()]\n",
    "                        if lines:\n",
    "                            if not s_val:\n",
    "                                s_val = lines[0]\n",
    "                            if len(lines) > 1 and not i_val:\n",
    "                                i_val = lines[1]\n",
    "                            if len(lines) > 2 and not d_val:\n",
    "                                d_val = lines[2]\n",
    "\n",
    "                    def finalize(x):\n",
    "                        if x is None:\n",
    "                            return ''\n",
    "                        t = str(x).strip()\n",
    "                        t = re.sub(r'^\\s*(suggestion|insight|drawback)\\b\\s*[:\\-–—]\\s*', '', t, flags=re.IGNORECASE)\n",
    "                        t = t.strip(' \\'\"{},:')\n",
    "                        t = re.sub(r'\\s+', ' ', t).strip()\n",
    "                        if len(t) > 500:\n",
    "                            t = t[:497] + \"...\"\n",
    "                        return t\n",
    "\n",
    "                    res.loc[res['RowID']==rid, 'AI_Suggestion'] = finalize(s_val)\n",
    "                    res.loc[res['RowID']==rid, 'AI_insights'] = finalize(i_val)\n",
    "                    res.loc[res['RowID']==rid, 'AI_Drawbacks'] = finalize(d_val)\n",
    "\n",
    "                try:\n",
    "                    # merge res into existing cache as well (so categories saved earlier remain)\n",
    "                    if os.path.exists(OUTPUT_CSV):\n",
    "                        try:\n",
    "                            cache_df = pd.read_csv(OUTPUT_CSV)\n",
    "                        except Exception:\n",
    "                            cache_df = pd.DataFrame()\n",
    "                    else:\n",
    "                        cache_df = pd.DataFrame()\n",
    "                    # join res rows into cache_df\n",
    "                    res_up = res.copy()\n",
    "                    if 'RowID' in cache_df.columns:\n",
    "                        cache_df = cache_df.set_index('RowID')\n",
    "                        res_up = res_up.set_index('RowID')\n",
    "                        for rid in res_up.index:\n",
    "                            cache_df.loc[rid, res_up.columns] = res_up.loc[rid]\n",
    "                        merged_cache = cache_df.reset_index()\n",
    "                    else:\n",
    "                        merged_cache = res_up.reset_index()\n",
    "                    merged_cache.to_csv(OUTPUT_CSV, index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # merge back into df\n",
    "        res = res.set_index('RowID')\n",
    "        df['AI_Suggestion'] = df['RowID'].map(lambda x: res.at[x, 'AI_Suggestion'] if x in res.index else '')\n",
    "        df['AI_insights'] = df['RowID'].map(lambda x: res.at[x, 'AI_insights'] if x in res.index else '')\n",
    "        df['AI_Drawbacks'] = df['RowID'].map(lambda x: res.at[x, 'AI_Drawbacks'] if x in res.index else '')\n",
    "\n",
    "        # remove helper RowID and return\n",
    "        dataset = df.drop(columns=['RowID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e701b72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32663a6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faef1b05",
   "metadata": {},
   "source": [
    "## This below code create the suggestions column like top 5 from the Ai_Suggestion, AI_insights, AI_Drawbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c607c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Power BI -> Top-5 suggestions (single column 'suggestion')\n",
    "# Uses only existing columns: AI_Suggestion, AI_insights, AI_Drawbacks as LLM input context.\n",
    "# Requires local LLM batch endpoint at SERVER_BATCH returning text in body (may include code fences).\n",
    "\n",
    "import pandas as pd, requests, json, re, os, time, ast\n",
    "from math import ceil\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "SERVER_BATCH = \"http://127.0.0.1:7860/batch_generate\"\n",
    "HEALTH = \"http://127.0.0.1:7860/health\"\n",
    "INSIGHTS_AUDIT_CSV = r\"E:\\AJAY\\powerbi_codes\\llm_top5_insights.csv\"\n",
    "# OUTPUT_CACHE_JSON removed per request (no cache.json file writing)\n",
    "TIMEOUT = 300\n",
    "RETRIES = 2\n",
    "RETRY_WAIT = 2\n",
    "BATCH_SIZE = 1\n",
    "MAX_TOKENS = 256   # reduced token budget per your ask\n",
    "TEMPERATURE = 0.0\n",
    "N_SAMPLE_ROWS = 40  # sample up to 40 rows from the three columns to build context\n",
    "# ----------------------------\n",
    "\n",
    "def safe_str(x):\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def build_context_sample(df, n=N_SAMPLE_ROWS):\n",
    "    parts = []\n",
    "    cnt = 0\n",
    "    for _, r in df.iterrows():\n",
    "        if cnt >= n:\n",
    "            break\n",
    "        s = safe_str(r.get('AI_Suggestion','')).replace('\\n',' ').strip()\n",
    "        i = safe_str(r.get('AI_insights','')).replace('\\n',' ').strip()\n",
    "        d = safe_str(r.get('AI_Drawbacks','')).replace('\\n',' ').strip()\n",
    "        entry_parts = []\n",
    "        if s: entry_parts.append(\"S:\" + (s[:300]))\n",
    "        if i: entry_parts.append(\"I:\" + (i[:300]))\n",
    "        if d: entry_parts.append(\"D:\" + (d[:300]))\n",
    "        if entry_parts:\n",
    "            parts.append(\" | \".join(entry_parts))\n",
    "            cnt += 1\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    joined = \" ||| \".join(parts)\n",
    "    return joined[:2400]\n",
    "\n",
    "def call_batch(payload):\n",
    "    try:\n",
    "        r = requests.post(SERVER_BATCH, json=payload, timeout=TIMEOUT)\n",
    "        return r\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def remove_code_fences_and_cleanup(s):\n",
    "    if not s or not isinstance(s, str):\n",
    "        return s\n",
    "    s = re.sub(r'```(?:json)?\\s*', '', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\s*```', '', s)\n",
    "    s = re.sub(r'~~~(?:json)?\\s*', '', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\s*~~~', '', s)\n",
    "    s = s.replace('\\\\n', '\\n').replace('\\\\\\\"', '\"').replace('\\\\\"', '\"')\n",
    "    return s\n",
    "\n",
    "def try_parse_json_candidates(s):\n",
    "    if not s or not isinstance(s, str):\n",
    "        return None\n",
    "    s0 = s.strip()\n",
    "    try:\n",
    "        obj = json.loads(s0)\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "        if isinstance(obj, dict) and 'results' in obj and isinstance(obj['results'], list):\n",
    "            return obj['results']\n",
    "        if isinstance(obj, dict) and any(k.lower().startswith('suggest') for k in obj.keys()):\n",
    "            return [obj]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cleaned = remove_code_fences_and_cleanup(s0)\n",
    "    m = re.search(r'(\\[[\\s\\S]*?\\])', cleaned)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        try:\n",
    "            obj = json.loads(cand)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            try:\n",
    "                obj = json.loads(cand.replace(\"'\", '\"'))\n",
    "                if isinstance(obj, list):\n",
    "                    return obj\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    objs = re.findall(r'(\\{[\\s\\S]*?\\})', cleaned)\n",
    "    parsed_objs = []\n",
    "    for o in objs:\n",
    "        try:\n",
    "            parsed = json.loads(o)\n",
    "            parsed_objs.append(parsed)\n",
    "        except Exception:\n",
    "            try:\n",
    "                parsed = json.loads(o.replace(\"'\", '\"'))\n",
    "                parsed_objs.append(parsed)\n",
    "            except Exception:\n",
    "                pass\n",
    "    if parsed_objs:\n",
    "        return parsed_objs\n",
    "\n",
    "    try:\n",
    "        val = ast.literal_eval(cleaned)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        if isinstance(val, dict) and 'results' in val and isinstance(val['results'], list):\n",
    "            return val['results']\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_text_candidates(body):\n",
    "    texts = []\n",
    "    if body is None:\n",
    "        return texts\n",
    "    if isinstance(body, dict):\n",
    "        for key in ('results','text','output','raw','generated_text','generation','choices'):\n",
    "            if key in body and body[key]:\n",
    "                texts.append(body[key])\n",
    "        if 'results' in body and isinstance(body['results'], list):\n",
    "            for it in body['results']:\n",
    "                if isinstance(it, dict):\n",
    "                    for k in ('text','raw','output'):\n",
    "                        if k in it and it[k]:\n",
    "                            texts.append(it[k])\n",
    "                    texts.append(json.dumps(it, ensure_ascii=False))\n",
    "                else:\n",
    "                    texts.append(str(it))\n",
    "        if 'choices' in body and isinstance(body['choices'], list):\n",
    "            for ch in body['choices']:\n",
    "                if isinstance(ch, dict):\n",
    "                    if 'text' in ch and ch['text']:\n",
    "                        texts.append(ch['text'])\n",
    "                    elif 'message' in ch and isinstance(ch['message'], dict):\n",
    "                        cont = ch['message'].get('content')\n",
    "                        if cont:\n",
    "                            texts.append(cont)\n",
    "                    else:\n",
    "                        texts.append(json.dumps(ch, ensure_ascii=False))\n",
    "                else:\n",
    "                    texts.append(str(ch))\n",
    "        texts.append(json.dumps(body, ensure_ascii=False))\n",
    "    elif isinstance(body, list):\n",
    "        texts.append(json.dumps(body, ensure_ascii=False))\n",
    "        for it in body:\n",
    "            texts.append(it if isinstance(it, str) else json.dumps(it, ensure_ascii=False))\n",
    "    else:\n",
    "        texts.append(str(body))\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for t in texts:\n",
    "        if t is None:\n",
    "            continue\n",
    "        s = t if isinstance(t, str) else json.dumps(t, ensure_ascii=False)\n",
    "        if s not in seen:\n",
    "            out.append(s)\n",
    "            seen.add(s)\n",
    "    return out\n",
    "\n",
    "def clean_suggestion_text(s):\n",
    "    if not s:\n",
    "        return ''\n",
    "    t = str(s).strip()\n",
    "    t = re.sub(r'^[\\s\"\\']+', '', t)\n",
    "    t = re.sub(r'[\\s\"\\']+$', '', t)\n",
    "    t = re.sub(r'\\bPriority\\b.*$', '', t, flags=re.IGNORECASE).strip()\n",
    "    toks = t.split()\n",
    "    if len(toks) > 15:\n",
    "        return \" \".join(toks[:15]) + \"...\"\n",
    "    return t\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "df = dataset.copy()\n",
    "\n",
    "for c in ('AI_Suggestion','AI_insights','AI_Drawbacks'):\n",
    "    if c not in df.columns:\n",
    "        df[c] = ''\n",
    "\n",
    "# If INSIGHTS_AUDIT_CSV exists and has cached suggestions, use it and skip LLM call\n",
    "insights_table = pd.DataFrame([{'suggestion':''} for _ in range(5)], columns=['suggestion'])\n",
    "try:\n",
    "    if os.path.exists(INSIGHTS_AUDIT_CSV):\n",
    "        try:\n",
    "            cached = pd.read_csv(INSIGHTS_AUDIT_CSV)\n",
    "            if 'suggestion' in cached.columns:\n",
    "                # check if there are >=1 non-empty suggestions\n",
    "                non_empty = cached['suggestion'].astype(str).str.strip().replace('', pd.NA).dropna()\n",
    "                if len(non_empty) >= 1:\n",
    "                    # load up to 5 rows from cache but coerce NaN/'nan'/None to empty string\n",
    "                    out_rows = []\n",
    "                    for i in range(5):\n",
    "                        if i < len(cached):\n",
    "                            raw_val = cached.iloc[i].get('suggestion','')\n",
    "                            # coerce NaN/None/'nan' to empty string\n",
    "                            if pd.isna(raw_val) or str(raw_val).strip().lower() in ('nan','none','na','n/a',''):\n",
    "                                val = ''\n",
    "                            else:\n",
    "                                val = str(raw_val).strip()\n",
    "                            out_rows.append({'suggestion': val})\n",
    "                        else:\n",
    "                            out_rows.append({'suggestion': ''})\n",
    "                    insights_table = pd.DataFrame(out_rows, columns=['suggestion'])\n",
    "                    dataset = df\n",
    "                else:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# If insights_table already loaded from cache (non-empty), we can skip heavy LLM call:\n",
    "if not (insights_table['suggestion'].astype(str).str.strip().replace('', pd.NA).dropna().empty):\n",
    "    # ensure no 'nan' strings remain\n",
    "    insights_table['suggestion'] = insights_table['suggestion'].apply(lambda x: '' if pd.isna(x) or str(x).strip().lower() in ('nan','none','na','n/a','') else str(x).strip())\n",
    "    try:\n",
    "        insights_table.to_csv(INSIGHTS_AUDIT_CSV, index=False, encoding='utf-8')\n",
    "    except Exception:\n",
    "        pass\n",
    "    dataset = df\n",
    "    insights_table = insights_table.reset_index(drop=True)\n",
    "else:\n",
    "    # Proceed to call LLM (same logic as before)\n",
    "    context_snip = build_context_sample(df)\n",
    "\n",
    "    PROMPT = (\n",
    "        \"You are an operations improvement expert. Read the supplied text context (three short AI columns) \"\n",
    "        \"and produce EXACTLY 5 distinct, concise, actionable suggestions. RETURN ONLY a JSON array of 5 objects; \"\n",
    "        \"each object MUST have precisely one key: \\\"suggestion\\\" (lowercase). \"\n",
    "        \"Each suggestion should be actionable and approximately 10-15 words. \"\n",
    "        \"Do NOT include reasons, priorities, numbering, or any commentary. Maintain strictly valid JSON array only.\\n\\n\"\n",
    "        f\"Context (AI_Suggestion / AI_insights / AI_Drawbacks samples): {context_snip}\\n\\nReturn JSON array now.\"\n",
    "    )\n",
    "\n",
    "    server_up = False\n",
    "    try:\n",
    "        h = requests.get(HEALTH, timeout=5)\n",
    "        server_up = (h.status_code == 200)\n",
    "    except Exception:\n",
    "        server_up = False\n",
    "\n",
    "    if server_up:\n",
    "        payload = {\"items\":[{\"prompt\":PROMPT, \"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE}]}\n",
    "        resp_body = None\n",
    "        try:\n",
    "            r = requests.post(SERVER_BATCH, json=payload, timeout=TIMEOUT)\n",
    "            if r is not None and r.status_code == 200:\n",
    "                try:\n",
    "                    resp_body = r.json()\n",
    "                except Exception:\n",
    "                    resp_body = r.text\n",
    "            else:\n",
    "                resp_body = None\n",
    "        except Exception:\n",
    "            resp_body = None\n",
    "\n",
    "        if resp_body is not None:\n",
    "            candidates = extract_text_candidates(resp_body)\n",
    "            parsed = None\n",
    "            for c in candidates:\n",
    "                if not c:\n",
    "                    continue\n",
    "                cleaned = remove_code_fences_and_cleanup(c)\n",
    "                parsed_candidate = try_parse_json_candidates(cleaned)\n",
    "                if parsed_candidate:\n",
    "                    parsed = parsed_candidate\n",
    "                    break\n",
    "                cleaned2 = re.sub(r'\\\\{2,}', '\\\\', cleaned)\n",
    "                parsed_candidate = try_parse_json_candidates(cleaned2)\n",
    "                if parsed_candidate:\n",
    "                    parsed = parsed_candidate\n",
    "                    break\n",
    "\n",
    "            suggestions = []\n",
    "            if isinstance(parsed, list) and parsed:\n",
    "                for el in parsed:\n",
    "                    if isinstance(el, dict) and 'suggestion' in el:\n",
    "                        txt = clean_suggestion_text(el.get('suggestion',''))\n",
    "                        suggestions.append(txt)\n",
    "                    elif isinstance(el, str):\n",
    "                        suggestions.append(clean_suggestion_text(el))\n",
    "                    elif isinstance(el, dict):\n",
    "                        for v in el.values():\n",
    "                            if isinstance(v, str) and v.strip():\n",
    "                                suggestions.append(clean_suggestion_text(v))\n",
    "                                break\n",
    "                    if len(suggestions) >= 5:\n",
    "                        break\n",
    "\n",
    "            final = []\n",
    "            seen = set()\n",
    "            for s in suggestions:\n",
    "                if s and s not in seen:\n",
    "                    final.append({'suggestion': s})\n",
    "                    seen.add(s)\n",
    "                if len(final) >= 5:\n",
    "                    break\n",
    "\n",
    "            while len(final) < 5:\n",
    "                final.append({'suggestion': ''})\n",
    "\n",
    "            if final:\n",
    "                insights_table = pd.DataFrame(final, columns=['suggestion'])\n",
    "\n",
    "    # persist for audit\n",
    "    try:\n",
    "        # ensure no NaN strings before saving\n",
    "        insights_table['suggestion'] = insights_table['suggestion'].apply(lambda x: '' if pd.isna(x) or str(x).strip().lower() in ('nan','none','na','n/a','') else str(x).strip())\n",
    "        insights_table.to_csv(INSIGHTS_AUDIT_CSV, index=False, encoding='utf-8')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dataset = df\n",
    "    insights_table = insights_table.reset_index(drop=True)\n",
    "\n",
    "# Final outputs (Power BI will pick these variables)\n",
    "dataset = dataset\n",
    "insights_table = insights_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40903972",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14476946",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28befa9",
   "metadata": {},
   "source": [
    "## SCRIPT A — Referrals_Clean (cleaned fact table + derived columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650e24f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "# If your source dates are dd/mm/yyyy set DAYFIRST = True\n",
    "DAYFIRST = False\n",
    "\n",
    "# Power Query supplies the table as 'dataset'\n",
    "df = dataset.copy()\n",
    "\n",
    "# 1) Parse date columns\n",
    "date_cols = [\n",
    "    \"Received Date\",\"First Administration Date\",\"Last Administration Date in Period\",\n",
    "    \"Service Provided Date\",\"Status Start Date\",\"Status End Date\",\n",
    "    \"Therapy Discontinuation Date\",\"Date Archived\"\n",
    "]\n",
    "for c in date_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=DAYFIRST)\n",
    "\n",
    "# 2) Trim/normalize text columns and fill simple nulls with \"Unknown\"\n",
    "text_cols = [\"Therapy Category\",\"KPI Frequency\",\"Drug Name\",\"Performance Metric\",\n",
    "             \"Current Patient Status\",\"Reason for Status\",\"Main Reason\",\"Insurance / Payer\"]\n",
    "for c in text_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip().replace({\"nan\": pd.NA, \"None\": pd.NA})\n",
    "        df[c] = df[c].fillna(\"Unknown\")\n",
    "\n",
    "# 3) Fill ONLY blank Main Reason with specific text\n",
    "if \"Main Reason\" in df.columns:\n",
    "    df[\"Main Reason\"] = df[\"Main Reason\"].replace([None, np.nan, \"\", \" \", \"nan\"], np.nan)\n",
    "    df.loc[df[\"Main Reason\"].isna(), \"Main Reason\"] = \"Active / Ongoing Therapy (No Reason Recorded)\"\n",
    "\n",
    "# 4) Fill ONLY blank Date Archived with Therapy Discontinuation Date + 10 days\n",
    "if (\"Date Archived\" in df.columns) and (\"Therapy Discontinuation Date\" in df.columns):\n",
    "    mask_fill = df[\"Date Archived\"].isna() & df[\"Therapy Discontinuation Date\"].notna()\n",
    "    df.loc[mask_fill, \"Date Archived\"] = df.loc[mask_fill, \"Therapy Discontinuation Date\"] + pd.Timedelta(days=10)\n",
    "    # keep Date Archived as datetime (do not convert to string here so Calendar can use it)\n",
    "    df[\"Date Archived\"] = pd.to_datetime(df[\"Date Archived\"], errors='coerce')\n",
    "\n",
    "# 5) Drop PII (First/Last Name) and anonymize Patient ID\n",
    "for c in [\"First Name\",\"Last Name\"]:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "if \"Patient ID\" in df.columns:\n",
    "    def anonymize(x):\n",
    "        try:\n",
    "            return hashlib.sha1(str(x).encode('utf-8')).hexdigest()[:12]\n",
    "        except:\n",
    "            return pd.NA\n",
    "    df[\"AnonymizedPatientID\"] = df[\"Patient ID\"].apply(anonymize)\n",
    "\n",
    "# 6) Derived numeric features (durations & flags)\n",
    "if set([\"First Administration Date\",\"Last Administration Date in Period\"]).issubset(df.columns):\n",
    "    df[\"Therapy_Duration_Days\"] = (df[\"Last Administration Date in Period\"] - df[\"First Administration Date\"]).dt.days\n",
    "\n",
    "if set([\"Received Date\",\"First Administration Date\"]).issubset(df.columns):\n",
    "    df[\"Time_to_First_Admin_Days\"] = (df[\"First Administration Date\"] - df[\"Received Date\"]).dt.days\n",
    "\n",
    "if set([\"Status Start Date\",\"Status End Date\"]).issubset(df.columns):\n",
    "    df[\"Status_Duration_Days\"] = (df[\"Status End Date\"] - df[\"Status Start Date\"]).dt.days\n",
    "\n",
    "if set([\"First Administration Date\",\"Therapy Discontinuation Date\"]).issubset(df.columns):\n",
    "    df[\"Days_To_Discontinuation\"] = (df[\"Therapy Discontinuation Date\"] - df[\"First Administration Date\"]).dt.days\n",
    "\n",
    "df[\"Is_Discontinued\"] = (df[\"Therapy Discontinuation Date\"].notna()).astype(int) if \"Therapy Discontinuation Date\" in df.columns else 0\n",
    "\n",
    "# 7) KPI Frequency -> KPI_Days mapping (normalize)\n",
    "kpi_map = {\"Daily\":1,\"24 hrs\":1,\"Weekly\":7,\"Monthly\":30,\"Quarterly\":90,\"Annual\":365,\"On Hold/Seasonal\":np.nan,\"On Hold\":np.nan,\"Unknown\":np.nan}\n",
    "if \"KPI Frequency\" in df.columns:\n",
    "    df[\"KPI_Freq_Normalized\"] = df[\"KPI Frequency\"].str.strip().replace({\n",
    "        r\"(?i)daily\":\"Daily\", r\"(?i)24[^0-9]*hrs?\":\"24 hrs\",\n",
    "        r\"(?i)weekly\":\"Weekly\", r\"(?i)monthly\":\"Monthly\",\n",
    "        r\"(?i)quarterly\":\"Quarterly\", r\"(?i)annual\":\"Annual\"\n",
    "    }, regex=True)\n",
    "    df[\"KPI_Days\"] = df[\"KPI_Freq_Normalized\"].map(kpi_map)\n",
    "\n",
    "# SLA breach flag\n",
    "if set([\"Time_to_First_Admin_Days\",\"KPI_Days\"]).issubset(df.columns):\n",
    "    df[\"SLA_Breach_Flag\"] = np.where((~df[\"Time_to_First_Admin_Days\"].isna()) & (~df[\"KPI_Days\"].isna()) & (df[\"Time_to_First_Admin_Days\"] > df[\"KPI_Days\"]),1,0)\n",
    "else:\n",
    "    df[\"SLA_Breach_Flag\"] = 0\n",
    "\n",
    "# 8) Therapy duration bucket\n",
    "bins = [-1,7,30,90,10**9]\n",
    "labels = [\"0-7 days\",\"8-30 days\",\"31-90 days\",\"90+ days\"]\n",
    "if \"Therapy_Duration_Days\" in df.columns:\n",
    "    df[\"Therapy_Duration_Bucket\"] = pd.cut(df[\"Therapy_Duration_Days\"].fillna(-1), bins=bins, labels=labels).astype(str)\n",
    "else:\n",
    "    df[\"Therapy_Duration_Bucket\"] = np.nan\n",
    "\n",
    "# 9) QA flag for status dates reversed\n",
    "if set([\"Status Start Date\",\"Status End Date\"]).issubset(df.columns):\n",
    "    df[\"StatusDate_Error\"] = np.where((~df[\"Status Start Date\"].isna()) & (~df[\"Status End Date\"].isna()) & (df[\"Status End Date\"] < df[\"Status Start Date\"]), \"EndBeforeStart\",\"\")\n",
    "\n",
    "# 10) Reorder columns (important ones first)\n",
    "front = [\"AnonymizedPatientID\",\"Patient ID\",\"Therapy Category\",\"Drug Name\",\"KPI Frequency\",\"KPI_Freq_Normalized\",\"KPI_Days\",\n",
    "         \"Performance Metric\",\"Received Date\",\"First Administration Date\",\"Last Administration Date in Period\",\n",
    "         \"Service Provided Date\",\"Current Patient Status\",\"Status Start Date\",\"Status End Date\",\"Status_Duration_Days\",\n",
    "         \"Reason for Status\",\"Therapy Discontinuation Date\",\"Days_To_Discontinuation\",\"Is_Discontinued\",\n",
    "         \"Main Reason\",\"Insurance / Payer\",\"Date Archived\",\"Therapy_Duration_Days\",\"Therapy_Duration_Bucket\",\n",
    "         \"Time_to_First_Admin_Days\",\"SLA_Breach_Flag\",\"StatusDate_Error\"]\n",
    "available = [c for c in front if c in df.columns]\n",
    "others = [c for c in df.columns if c not in available]\n",
    "df = df[available + others]\n",
    "\n",
    "# Return cleaned dataframe back to Power Query\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fcaf6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb86f1da",
   "metadata": {},
   "source": [
    "## SCRIPT B — Calendar_DateTable (daily calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c655c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dataset is Referrals_Clean\n",
    "df = dataset.copy()\n",
    "\n",
    "# date columns to inspect (only dates present)\n",
    "date_cols = [\n",
    "    \"Received Date\",\"First Administration Date\",\"Last Administration Date in Period\",\n",
    "    \"Service Provided Date\",\"Status Start Date\",\"Status End Date\",\n",
    "    \"Therapy Discontinuation Date\",\"Date Archived\"\n",
    "]\n",
    "date_cols = [c for c in date_cols if c in df.columns]\n",
    "\n",
    "# collect non-null date values (floor to date)\n",
    "dates = []\n",
    "for c in date_cols:\n",
    "    try:\n",
    "        dates += df[c].dropna().dt.floor('D').tolist()\n",
    "    except:\n",
    "        # fallback: coerce to datetime then take date\n",
    "        dates += pd.to_datetime(df[c], errors='coerce').dropna().dt.floor('D').tolist()\n",
    "\n",
    "# fallback if no dates found\n",
    "if len(dates) == 0:\n",
    "    mindate = pd.Timestamp.today().floor('D') - pd.Timedelta(days=365)\n",
    "    maxdate = pd.Timestamp.today().floor('D') + pd.Timedelta(days=365)\n",
    "else:\n",
    "    mindate = min(dates)\n",
    "    maxdate = max(dates)\n",
    "\n",
    "# build calendar (daily)\n",
    "cal = pd.DataFrame({\"Date\": pd.date_range(start=mindate, end=maxdate, freq='D')})\n",
    "cal[\"Year\"] = cal[\"Date\"].dt.year\n",
    "cal[\"Quarter\"] = cal[\"Date\"].dt.quarter\n",
    "cal[\"MonthNo\"] = cal[\"Date\"].dt.month\n",
    "cal[\"MonthName\"] = cal[\"Date\"].dt.strftime(\"%B\")\n",
    "cal[\"MonthShort\"] = cal[\"Date\"].dt.strftime(\"%b\")\n",
    "cal[\"YearMonth\"] = cal[\"Date\"].dt.strftime(\"%Y-%m\")\n",
    "cal[\"WeekNum\"] = cal[\"Date\"].dt.isocalendar().week\n",
    "cal[\"WeekDay\"] = cal[\"Date\"].dt.weekday + 1  # Mon=1..Sun=7\n",
    "cal[\"Day\"] = cal[\"Date\"].dt.day\n",
    "cal[\"DayName\"] = cal[\"Date\"].dt.strftime(\"%A\")\n",
    "cal[\"IsWeekend\"] = np.where(cal[\"WeekDay\"] > 5, 1, 0)\n",
    "cal[\"WeekEnding\"] = cal[\"Date\"] + pd.to_timedelta(7 - cal[\"WeekDay\"], unit='D')\n",
    "cal[\"QuarterNumber\"] = \"Q\" + cal[\"Quarter\"].astype(str)\n",
    "\n",
    "# Return calendar to Power Query\n",
    "cal.reset_index(drop=True, inplace=True)\n",
    "cal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd10742",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ccb81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8e02b2",
   "metadata": {},
   "source": [
    "## Step-by-Step to Create and Save the “Image Table”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2eb69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Copy from Power BI input\n",
    "df = dataset.copy()\n",
    "\n",
    "# Keep only Insurance / Payer\n",
    "df = df[['Insurance / Payer']].dropna()\n",
    "df['Insurance / Payer'] = df['Insurance / Payer'].astype(str).str.strip()\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset=['Insurance / Payer']).reset_index(drop=True)\n",
    "\n",
    "# Create Image ID (factorized)\n",
    "df[\"Image ID\"] = pd.factorize(df[\"Insurance / Payer\"])[0] + 1\n",
    "\n",
    "# Add empty Image column\n",
    "df[\"Image\"] = \"\"\n",
    "\n",
    "# Reorder columns\n",
    "df = df[[\"Image ID\", \"Insurance / Payer\", \"Image\"]]\n",
    "\n",
    "# Return the table to Power BI\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632697e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c518a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0b1255a",
   "metadata": {},
   "source": [
    "## Storing Large Images In Power BI Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642489e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "  //Get list of files in folder \n",
    "  Source = Folder.Files(\"C:\\Users\\Chris\\Documents\\PQ Pics\"),\n",
    "  //Remove unnecessary columns \n",
    "  RemoveOtherColumns = Table.SelectColumns(Source, {\"Content\", \"Name\"}),\n",
    "  //Creates Splitter function \n",
    "  SplitTextFunction = Splitter.SplitTextByRepeatedLengths(30000),\n",
    "  //Converts table of files to list \n",
    "  ListInput = Table.ToRows(RemoveOtherColumns),\n",
    "  //Function to convert binary of photo to multiple \n",
    "  //text values \n",
    "  ConvertOneFile = (InputRow as list) =>\n",
    "    let\n",
    "      BinaryIn    = InputRow{0},\n",
    "      FileName    = InputRow{1},\n",
    "      BinaryText  = Binary.ToText(BinaryIn, BinaryEncoding.Base64),\n",
    "      SplitUpText = SplitTextFunction(BinaryText),\n",
    "      AddFileName = List.Transform(SplitUpText, each {FileName, _})\n",
    "    in\n",
    "      AddFileName,\n",
    "  //Loops over all photos and calls the above function \n",
    "  ConvertAllFiles = List.Transform(ListInput, each ConvertOneFile(_)),\n",
    "  //Combines lists together \n",
    "  CombineLists = List.Combine(ConvertAllFiles),\n",
    "  //Converts results to table \n",
    "  ToTable = #table(type table [Name = text, Pic = text], CombineLists),\n",
    "  //Adds index column to output table \n",
    "  AddIndexColumn = Table.AddIndexColumn(ToTable, \"Index\", 0, 1)\n",
    "in\n",
    "  AddIndexColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d0ea4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Display Image = \n",
    "IF( HASONEVALUE('PQ Pics'[Name]), \n",
    "\"data:image/jpeg;base64, \" & \n",
    "CONCATENATEX( 'PQ Pics', 'PQ Pics'[Pic], , 'PQ Pics'[Index], ASC) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569d423",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f03f23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb6aac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369318b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c056c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
